{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b1c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9714d55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1119bb0e3c9a4a848b21d310d163073d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff1bd00d045480aa6b11a467066ccd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bff744508f4e1da48baae127b0993b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4419ccf3b14840e0b3298b75c2ebe218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/889 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92626c5793a64751aa193d98d1890f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7bcfd8d6ee4719a9f7abe1abbcfa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee109a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "\n",
      "model.layers.0.input_layernorm.weight\n",
      "\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "\n",
      "model.layers.1.input_layernorm.weight\n",
      "\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "\n",
      "model.layers.2.input_layernorm.weight\n",
      "\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "\n",
      "model.layers.3.input_layernorm.weight\n",
      "\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "\n",
      "model.layers.4.input_layernorm.weight\n",
      "\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "\n",
      "model.layers.5.input_layernorm.weight\n",
      "\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "\n",
      "model.layers.6.input_layernorm.weight\n",
      "\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "\n",
      "model.layers.7.input_layernorm.weight\n",
      "\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "\n",
      "model.layers.8.input_layernorm.weight\n",
      "\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "\n",
      "model.layers.9.input_layernorm.weight\n",
      "\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "\n",
      "model.layers.10.input_layernorm.weight\n",
      "\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "\n",
      "model.layers.11.input_layernorm.weight\n",
      "\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "\n",
      "model.layers.12.input_layernorm.weight\n",
      "\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "\n",
      "model.layers.13.input_layernorm.weight\n",
      "\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "\n",
      "model.layers.14.input_layernorm.weight\n",
      "\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "\n",
      "model.layers.15.input_layernorm.weight\n",
      "\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "\n",
      "model.norm.weight\n",
      "\n",
      "lm_head.weight\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in model.state_dict().keys():\n",
    "    print(f\"{key}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c93093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1334)\n",
    "\n",
    "x = torch.rand(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45d13b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents implementation of RMSNorm layer. PyTorch offers layer RMSNorm layer but I will implement it from scratch for educational purposes \n",
    "    \"\"\"\n",
    "    def __init__(self, embd_dim, epsilon: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.embd_dim = embd_dim\n",
    "        self.weight = nn.Parameter(torch.ones(embd_dim)).float()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        root_mean_squares = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.epsilon)\n",
    "        x_normalized = x / root_mean_squares\n",
    "        return (x_normalized * self.weight).to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c7713a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee418261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0407, 1.0997, 1.2548, 1.1020],\n",
       "         [1.4200, 0.8225, 0.1628, 1.1317],\n",
       "         [0.1437, 1.6313, 0.6554, 0.9426]],\n",
       "\n",
       "        [[1.5116, 1.1063, 0.0312, 0.7000],\n",
       "         [1.3568, 1.3647, 0.4776, 0.2612],\n",
       "         [0.4313, 0.2693, 1.5692, 1.1309]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsl = nn.RMSNorm(normalized_shape=x.shape[-1], eps=1e-5)\n",
    "rmsl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b950bbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0407, 1.0997, 1.2548, 1.1020],\n",
       "         [1.4200, 0.8225, 0.1628, 1.1317],\n",
       "         [0.1437, 1.6313, 0.6554, 0.9426]],\n",
       "\n",
       "        [[1.5116, 1.1063, 0.0312, 0.7000],\n",
       "         [1.3568, 1.3647, 0.4776, 0.2612],\n",
       "         [0.4313, 0.2693, 1.5692, 1.1309]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rmsl = RMSNormLayer(embd_dim=x.shape[-1], epsilon=1e-5)\n",
    "my_rmsl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0875f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim):\n",
    "    # intiger -> binary -> sinusoidal -> RoPE\n",
    "    # RoPE does not polute the semantics of token embeddings because we do not add vector containing the positional info, we rotate queries and keys\n",
    "    # Encodes both absolute and relative position information\n",
    "    # theta = position * omega\n",
    "    # omega = 1 / 500000 ^ (2i/dim)\n",
    "    # rot matrix = [[cos(theta) -sin(theta)] [sin(theta) cos(theta)]]\n",
    "    # Token embedding gets split into groups of 2 so there are dim // 2 groups of 2 that get rotated\n",
    "    # Lower indeces in token embeddings oscilate more quickly capturing small changes\n",
    "    position = torch.arange(0, 100).view(100,1)\n",
    "    print(f'{position.shape}')\n",
    "    i = torch.arange(0, head_dim, 2, dtype=torch.float32) # (head_dim/2, )\n",
    "    print(f'{i.shape}')\n",
    "    omega = 1.0 / torch.pow(10000, (i / head_dim)).view(1, int(head_dim/2)) # (1, head_dim/2)\n",
    "    print(f'{omega.shape}')\n",
    "    theta = position * omega\n",
    "    # Theta has to be exapneded to each index because right now it only holds values for even indeces -> or maybe not if I do not want to have double the params - ROPEFormer uses only half\n",
    "    print(theta.shape)\n",
    "    return torch.cos(theta), torch.sin(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66e43232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([50])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([100, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.5403,  0.6736,  0.7701,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [-0.4161, -0.0926,  0.1860,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         ...,\n",
       "         [-0.9251,  0.5400, -0.4229,  ...,  0.9999,  0.9999,  0.9999],\n",
       "         [-0.8193,  0.9858,  0.2524,  ...,  0.9999,  0.9999,  0.9999],\n",
       "         [ 0.0398,  0.7880,  0.8117,  ...,  0.9999,  0.9999,  0.9999]]),\n",
       " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 8.4147e-01,  7.3912e-01,  6.3795e-01,  ...,  1.7378e-04,\n",
       "           1.4454e-04,  1.2023e-04],\n",
       "         [ 9.0930e-01,  9.9570e-01,  9.8254e-01,  ...,  3.4756e-04,\n",
       "           2.8909e-04,  2.4045e-04],\n",
       "         ...,\n",
       "         [ 3.7961e-01, -8.4164e-01, -9.0618e-01,  ...,  1.6856e-02,\n",
       "           1.4020e-02,  1.1662e-02],\n",
       "         [-5.7338e-01, -1.6776e-01, -9.6761e-01,  ...,  1.7030e-02,\n",
       "           1.4165e-02,  1.1782e-02],\n",
       "         [-9.9921e-01,  6.1565e-01, -5.8410e-01,  ...,  1.7203e-02,\n",
       "           1.4309e-02,  1.1902e-02]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precompute_rope_params(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc72692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_simulation = torch.arange(0, 100).reshape((50, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf5318b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5],\n",
       "          [ 6,  7],\n",
       "          [ 8,  9],\n",
       "          [10, 11],\n",
       "          [12, 13],\n",
       "          [14, 15],\n",
       "          [16, 17],\n",
       "          [18, 19],\n",
       "          [20, 21],\n",
       "          [22, 23],\n",
       "          [24, 25],\n",
       "          [26, 27],\n",
       "          [28, 29],\n",
       "          [30, 31],\n",
       "          [32, 33],\n",
       "          [34, 35],\n",
       "          [36, 37],\n",
       "          [38, 39]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = cos_simulation[:20, :].reshape((1, 1, 20, 2))\n",
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef7f515e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0,  -1],\n",
       "          [ -2,  -3],\n",
       "          [ -4,  -5],\n",
       "          [ -6,  -7],\n",
       "          [ -8,  -9],\n",
       "          [-10, -11],\n",
       "          [-12, -13],\n",
       "          [-14, -15],\n",
       "          [-16, -17],\n",
       "          [-18, -19],\n",
       "          [-20, -21],\n",
       "          [-22, -23],\n",
       "          [-24, -25],\n",
       "          [-26, -27],\n",
       "          [-28, -29],\n",
       "          [-30, -31],\n",
       "          [-32, -33],\n",
       "          [-34, -35],\n",
       "          [-36, -37],\n",
       "          [-38, -39]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 * cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c922b134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  2],\n",
       "         [ 4,  6],\n",
       "         [ 8, 10]],\n",
       "\n",
       "        [[ 1,  3],\n",
       "         [ 5,  7],\n",
       "         [ 9, 11]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default dim for stacking is 0\n",
    "torch.stack((torch.arange(0, 12, 2).view(3,2), torch.arange(1, 12, 2).view(3,2)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ff8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((torch.arange(0, 12, 2).view(3,2), torch.arange(1, 12, 2).view(3,2)), dim=0).shape\n",
    "# 3D tensor - 2 batches, each batch matrix with 3 rows and 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaa61e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  2],\n",
       "         [ 1,  3]],\n",
       "\n",
       "        [[ 4,  6],\n",
       "         [ 5,  7]],\n",
       "\n",
       "        [[ 8, 10],\n",
       "         [ 9, 11]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((torch.arange(0, 12, 2).view(3,2), torch.arange(1, 12, 2).view(3,2)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6de539d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((torch.arange(0, 12, 2).view(3,2), torch.arange(1, 12, 2).view(3,2)), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c42ef22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3]],\n",
       "\n",
       "        [[ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((torch.arange(0, 12, 2).view(3,2), torch.arange(1, 12, 2).view(3,2)), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cc69037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((torch.arange(0, 12, 2).view(3,2), torch.arange(1, 12, 2).view(3,2)), dim=2).reshape(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34978ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.arange(0, 5), torch.arange(5, 10)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d04b5138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(torch.arange(0, 10), 2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "297dfc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.arange(0, 12).view(3, 4)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b507fb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  1,  1,  2,  2,  3,  3],\n",
       "        [ 4,  4,  5,  5,  6,  6,  7,  7],\n",
       "        [ 8,  8,  9,  9, 10, 10, 11, 11]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.repeat_interleave(2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71ca6e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7,  4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11,  8,  9, 10, 11],\n",
       "        [ 0,  1,  2,  3,  0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7,  4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.repeat(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9988285c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248158bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.+1.j, 2.+3.j, 4.+5.j, 6.+7.j, 8.+9.j])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp = torch.view_as_complex(torch.arange(10, dtype=torch.float).view(-1, 2))\n",
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3b54df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.],\n",
       "        [6., 7.],\n",
       "        [8., 9.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.view_as_real(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d67eab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.zeros(10, 10, device=x.device, dtype=torch.bool), diagonal=1)[5:7, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c9f5a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63e6f638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1,10).masked_fill(mask, -torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0c7d268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10).view(2,5)[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6959b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 500).view(10, 50)[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6054cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
